# ==============================================
# Model Configuration
# ==============================================
base_model: Qwen/Qwen2.5-1.5B-Instruct
tokenizer_type: Qwen/Qwen2.5-1.5B-Instruct

# ==============================================
# Dataset
# ==============================================
dataset:
  name: skadewdl3/recipe-nlg-llama2
  cache_dir: ../data/datasets
  field_map:
    input: NER  # Named entity recognition (ingredients list)
    output: directions  # Recipe directions (what we're generating)
    title: title  # Recipe title
    ingredients: ingredients  # Full ingredient list
  type: completion
  splits:
    train: all
    validation: 200
    test: 200
  seed: 42
  val_size: 0.05  # Fraction for validation split creation

# This is for Axolotl
datasets:
  - path: skadewdl3/recipe-nlg-llama2
    cache_dir: ../data/datasets
    field_map:
      input: NER
      output: directions
      title: title
      ingredients: ingredients
    type: completion

task_instruction: >
  You will generate one cooking recipe. List all necessary ingredients and give detailed steps.
  Include the following ingredients:

train_samples: all
val_samples: 200
test_samples: 200
seed: 42

# ==============================================
# Quantization (for QLoRA)
# ==============================================
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: bfloat16

# ==============================================
# LoRA Configuration
# ==============================================
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
target_modules: ["q_proj", "v_proj"]

# ==============================================
# Training Configuration
# ==============================================
num_epochs: 1
max_steps: 50  # Reduced for DDP testing - increase after it works
learning_rate: 2e-4
batch_size: 2  # Reduced for DDP stability - can increase after testing
gradient_accumulation_steps: 8  # Increased to compensate for smaller batch_size
sequence_len: 512 # works well for the recipe dataset
lr_scheduler: cosine
warmup_steps: 25  # Reduced proportionally with max_steps
bf16: true
logging_steps: 10  # More frequent logging to monitor progress
save_steps: 50  # Save at end of this test run
save_total_limit: 2
optim: paged_adamw_8bit
gradient_checkpointing: true  # Enable to reduce memory and potentially help with DDP

# ==============================================
# Output & Logging
# ==============================================
output_dir: ./outputs/lora_recipe
wandb_project: qwen_recipe
wandb_run_name: lora-finetuning-recipe
hub_model_name: Qwen2.5-1.5B-QLoRA-Recipe 
