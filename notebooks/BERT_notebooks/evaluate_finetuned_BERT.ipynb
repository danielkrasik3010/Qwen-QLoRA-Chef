{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -q torch tqdm datasets peft transformers bert-score\n",
        "! pip install -U bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from bert_score import score as bert_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Configuration\n",
        "# =============================================================================\n",
        "DATASETS_DIR = \"./datasets\"\n",
        "\n",
        "# Fine-tuned model from HuggingFace Hub\n",
        "ADAPTER_ID = \"Daniel-Krasik/Qwen2.5-1.5B-QLoRA-Recipe\"\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration registry\n",
        "MODEL_CONFIGS = {\n",
        "    \"llama\": {\n",
        "        \"path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "        \"supports_system\": True,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"mistral\": {\n",
        "        \"path\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "        \"supports_system\": False,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"gemma\": {\n",
        "        \"path\": \"ggoogle/gemma-2-2b-it\",\n",
        "        \"supports_system\": False,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"qwen\": {\n",
        "        \"path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        \"supports_system\": True,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"olmo\": {\n",
        "        \"path\": \"allenai/OLMoE-1B-7B-0924-Instruct\",\n",
        "        \"supports_system\": False,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "def get_model_config_from_path(model_path: str):\n",
        "    \"\"\"Extract model configuration from full model path.\"\"\"\n",
        "    model_path_lower = model_path.lower()\n",
        "\n",
        "    if \"llama\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"llama\"].copy()\n",
        "    elif \"mistral\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"mistral\"].copy()\n",
        "    elif \"gemma\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"gemma\"].copy()\n",
        "    elif \"qwen\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"qwen\"].copy()\n",
        "    elif \"olmo\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"olmo\"].copy()\n",
        "    else:\n",
        "        print(f\"[WARNING] Unknown model path: {model_path}. Using Llama format as default.\")\n",
        "        return MODEL_CONFIGS[\"llama\"].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_subset(dataset, n_samples, seed=42):\n",
        "    \"\"\"Select a subset of the dataset.\"\"\"\n",
        "    if n_samples == \"all\" or n_samples is None:\n",
        "        return dataset\n",
        "\n",
        "    if n_samples > len(dataset):\n",
        "        print(f\"[WARNING] Requested {n_samples} samples but only {len(dataset)} available. Using all samples.\")\n",
        "        return dataset\n",
        "\n",
        "    return dataset.shuffle(seed=seed).select(range(n_samples))\n",
        "\n",
        "\n",
        "def load_and_prepare_dataset(cfg):\n",
        "    \"\"\"Load dataset splits according to configuration.\"\"\"\n",
        "    # Extract dataset configuration\n",
        "    if \"dataset\" in cfg:\n",
        "        cfg_dataset = cfg[\"dataset\"]\n",
        "        dataset_name = cfg_dataset[\"name\"]\n",
        "        splits_cfg = cfg_dataset.get(\"splits\", {})\n",
        "        n_train = splits_cfg.get(\"train\", \"all\")\n",
        "        n_val = splits_cfg.get(\"validation\", \"all\")\n",
        "        n_test = splits_cfg.get(\"test\", \"all\")\n",
        "        seed = cfg_dataset.get(\"seed\", 42)\n",
        "    elif \"datasets\" in cfg and isinstance(cfg[\"datasets\"], list):\n",
        "        cfg_dataset = cfg[\"datasets\"][0]\n",
        "        dataset_name = cfg_dataset[\"path\"]\n",
        "        n_train = cfg.get(\"train_samples\", \"all\")\n",
        "        n_val = cfg.get(\"val_samples\", \"all\")\n",
        "        n_test = cfg.get(\"test_samples\", \"all\")\n",
        "        seed = cfg.get(\"seed\", 42)\n",
        "    else:\n",
        "        raise KeyError(\"Dataset configuration not found.\")\n",
        "\n",
        "    # Load or download full dataset\n",
        "    os.makedirs(DATASETS_DIR, exist_ok=True)\n",
        "    local_path = os.path.join(DATASETS_DIR, dataset_name.replace(\"/\", \"_\"))\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"[INFO] Loading dataset from local cache: {local_path}\")\n",
        "        dataset = load_from_disk(local_path)\n",
        "    else:\n",
        "        print(f\"[INFO] Downloading dataset from Hugging Face: {dataset_name}\")\n",
        "        dataset = load_dataset(dataset_name)\n",
        "        dataset.save_to_disk(local_path)\n",
        "        print(f\"[INFO] Full dataset saved locally to: {local_path}\")\n",
        "\n",
        "    # Filter invalid samples\n",
        "    def is_valid(sample):\n",
        "        return (\n",
        "            sample.get('title') is not None and str(sample.get('title', '')).strip() and\n",
        "            sample.get('ingredients') is not None and str(sample.get('ingredients', '')).strip() and\n",
        "            sample.get('directions') is not None and str(sample.get('directions', '')).strip() and\n",
        "            sample.get('prompt') is not None and str(sample.get('prompt', '')).strip() and\n",
        "            '[INST]' in str(sample.get('prompt', '')) and '[/INST]' in str(sample.get('prompt', ''))\n",
        "        )\n",
        "\n",
        "    print(\"\\n[INFO] Filtering Invalid Samples:\")\n",
        "    for split_name in dataset.keys():\n",
        "        original_size = len(dataset[split_name])\n",
        "        dataset[split_name] = dataset[split_name].filter(is_valid)\n",
        "        new_size = len(dataset[split_name])\n",
        "        removed = original_size - new_size\n",
        "        print(f\"  {split_name}: kept {new_size:,} / {original_size:,} (removed {removed:,})\")\n",
        "\n",
        "    # Create validation split if it doesn't exist\n",
        "    if \"validation\" not in dataset and \"val\" not in dataset:\n",
        "        val_size = cfg_dataset.get(\"val_size\", 0.05)\n",
        "        print(f\"\\n[INFO] Creating Validation Split ({val_size*100:.1f}% of train)\")\n",
        "        train_val_split = dataset['train'].train_test_split(test_size=val_size, seed=seed)\n",
        "        dataset['train'] = train_val_split['train']\n",
        "        dataset['validation'] = train_val_split['test']\n",
        "        print(f\"[INFO] Created validation split: {len(dataset['validation']):,} samples\")\n",
        "\n",
        "    val_key = \"validation\" if \"validation\" in dataset else \"val\"\n",
        "    train = select_subset(dataset[\"train\"], n_train, seed=seed)\n",
        "    val = select_subset(dataset[val_key], n_val, seed=seed)\n",
        "    test = select_subset(dataset[\"test\"], n_test, seed=seed)\n",
        "\n",
        "    print(f\"\\n[INFO] Loaded {len(train)} train / {len(val)} val / {len(test)} test samples.\")\n",
        "    return train, val, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_finetuned_model_and_tokenizer(base_model: str, adapter_id: str, use_4bit: bool = True):\n",
        "    \"\"\"\n",
        "    Load base model with 4-bit quantization and attach fine-tuned LoRA adapters from HuggingFace.\n",
        "    \n",
        "    Args:\n",
        "        base_model (str): Base model path (e.g., \"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "        adapter_id (str): HuggingFace adapter ID (e.g., \"Daniel-Krasik/Qwen2.5-1.5B-QLoRA-Recipe\")\n",
        "        use_4bit (bool): Whether to use 4-bit quantization (default: True)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "    print(f\"\\n[INFO] Loading base model: {base_model}\")\n",
        "\n",
        "    # Tokenizer setup\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    # Quantization setup\n",
        "    quant_cfg = None\n",
        "    if use_4bit:\n",
        "        print(\"[INFO] Enabling 4-bit quantization...\")\n",
        "        quant_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    else:\n",
        "        print(\"[INFO] Loading model in full precision.\")\n",
        "\n",
        "    # Load base model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        quantization_config=quant_cfg,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    )\n",
        "\n",
        "    # Load LoRA adapters from HuggingFace Hub\n",
        "    print(f\"[INFO] Loading fine-tuned LoRA adapters from HuggingFace: {adapter_id}\")\n",
        "    model = PeftModel.from_pretrained(model, adapter_id)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"[INFO] Fine-tuned model loaded successfully!\")\n",
        "    return model, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG_FILE_PATH = \"/content/master_config.yaml\"\n",
        "\n",
        "def load_config(config_path: str = CONFIG_FILE_PATH):\n",
        "    \"\"\"Load and parse a YAML configuration file.\"\"\"\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    return cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = load_config(CONFIG_FILE_PATH)\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_predictions(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset,\n",
        "    task_instruction,\n",
        "    cfg=None,\n",
        "    num_samples=None,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=1000,\n",
        "):\n",
        "    \"\"\"Generate model predictions for a dataset (recipe directions).\"\"\"\n",
        "    if num_samples is not None and num_samples < len(dataset):\n",
        "        dataset = dataset.select(range(num_samples))\n",
        "\n",
        "    # Get field names from config\n",
        "    if cfg is not None:\n",
        "        field_map = cfg.get(\"dataset\", {}).get(\"field_map\", {})\n",
        "        input_field = field_map.get(\"input\", \"NER\")\n",
        "        base_model = cfg.get(\"base_model\", \"\")\n",
        "    else:\n",
        "        input_field = \"NER\"\n",
        "        base_model = BASE_MODEL\n",
        "\n",
        "    # Get model config to determine message format\n",
        "    model_config = get_model_config_from_path(base_model)\n",
        "\n",
        "    # Prepare prompts\n",
        "    prompts = []\n",
        "    for sample in dataset:\n",
        "        messages = []\n",
        "\n",
        "        if model_config['supports_system']:\n",
        "            system_msg = {\"role\": \"system\", \"content\": model_config['system_message']}\n",
        "            messages.append(system_msg)\n",
        "            user_content = model_config['user_message_template'].format(ner=sample.get(input_field, ''))\n",
        "            user_msg = {\"role\": \"user\", \"content\": user_content}\n",
        "            messages.append(user_msg)\n",
        "        else:\n",
        "            user_lines = [model_config['system_message'], \"\"]\n",
        "            ner = sample.get(input_field, '')\n",
        "            user_content = model_config['user_message_template'].format(ner=ner)\n",
        "            user_lines.append(user_content)\n",
        "            user_msg = {\"role\": \"user\", \"content\": \"\\n\\n\".join(user_lines)}\n",
        "            messages.append(user_msg)\n",
        "\n",
        "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=\"auto\",\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    # Generate predictions\n",
        "    preds = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating recipes\"):\n",
        "        batch = prompts[i : i + batch_size]\n",
        "        outputs = pipe(batch, max_new_tokens=max_new_tokens, return_full_text=False)\n",
        "        preds.extend([o[0][\"generated_text\"].strip() for o in outputs])\n",
        "\n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_bert_score(predictions, samples, cfg=None, lang=\"en\", model_type=None, batch_size=32):\n",
        "    \"\"\"\n",
        "    Compute BERTScore between predictions and reference full recipe format.\n",
        "    \n",
        "    Args:\n",
        "        predictions (list[str]): Model-generated outputs.\n",
        "        samples (datasets.Dataset): Dataset containing recipe fields.\n",
        "        cfg (dict, optional): Configuration dictionary.\n",
        "        lang (str): Language for BERTScore (default: \"en\").\n",
        "        model_type (str, optional): Specific BERT model to use.\n",
        "        batch_size (int): Batch size for BERTScore computation.\n",
        "    \n",
        "    Returns:\n",
        "        dict: BERTScore precision, recall, and F1 scores.\n",
        "    \"\"\"\n",
        "    # Build full recipe format for references\n",
        "    references = []\n",
        "    for sample in samples:\n",
        "        full_recipe = (\n",
        "            f\"Certainly! Here's a delicious recipe for:\\n\"\n",
        "            f\"[ {sample.get('title', 'Recipe')} ]\\n\\n\"\n",
        "            f\"[ INGREDIENTS ]\\n{sample.get('ingredients', '')}\\n\\n\"\n",
        "            f\"[ DIRECTIONS ]\\n{sample.get('directions', '')}\"\n",
        "        )\n",
        "        references.append(full_recipe)\n",
        "\n",
        "    # Compute BERTScore\n",
        "    print(f\"\\n[INFO] Computing BERTScore with lang='{lang}', model_type='{model_type or 'default (roberta-large)'}', batch_size={batch_size}\")\n",
        "    P, R, F1 = bert_score(\n",
        "        cands=predictions,\n",
        "        refs=references,\n",
        "        lang=lang,\n",
        "        model_type=model_type,\n",
        "        verbose=True,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"bert_precision\": P.mean().item(),\n",
        "        \"bert_recall\": R.mean().item(),\n",
        "        \"bert_f1\": F1.mean().item(),\n",
        "        \"bert_precision_per_sample\": P.tolist(),\n",
        "        \"bert_recall_per_sample\": R.tolist(),\n",
        "        \"bert_f1_per_sample\": F1.tolist(),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "evaluate_finetuned_bert.ipynb\n",
        "Evaluate the fine-tuned model (from HuggingFace) on the recipe generation dataset using BERT Score.\n",
        "\"\"\"\n",
        "\n",
        "cfg = load_config()\n",
        "\n",
        "def evaluate_finetuned_bert():\n",
        "    \"\"\"Run evaluation on the recipe generation dataset using the fine-tuned model and BERT Score.\"\"\"\n",
        "\n",
        "    # Load validation data\n",
        "    _, val_data, _ = load_and_prepare_dataset(cfg)\n",
        "    print(f\"[INFO] Loaded {len(val_data)} validation samples.\")\n",
        "\n",
        "    # Load fine-tuned model directly from HuggingFace (merged model)\n",
        "    model, tokenizer = setup_finetuned_model_and_tokenizer(\n",
        "        model_id=FINETUNED_MODEL_ID,\n",
        "        use_4bit=True,\n",
        "    )\n",
        "\n",
        "    # Generate predictions\n",
        "    print(\"\\n[INFO] Generating recipes with fine-tuned model...\")\n",
        "    preds = generate_predictions(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=val_data,\n",
        "        task_instruction=cfg[\"task_instruction\"],\n",
        "        cfg=cfg,\n",
        "        batch_size=4,\n",
        "    )\n",
        "\n",
        "    # Compute BERT scores\n",
        "    print(\"\\n[INFO] Computing BERT scores...\")\n",
        "    bert_scores = compute_bert_score(preds, val_data, cfg=cfg)\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Save outputs\n",
        "    # -----------------------------------------------------------------------\n",
        "    results = {\n",
        "        \"finetuned_model_id\": FINETUNED_MODEL_ID,\n",
        "        \"base_model\": BASE_MODEL,\n",
        "        \"num_samples\": len(val_data),\n",
        "        \"bert_precision\": bert_scores[\"bert_precision\"],\n",
        "        \"bert_recall\": bert_scores[\"bert_recall\"],\n",
        "        \"bert_f1\": bert_scores[\"bert_f1\"],\n",
        "    }\n",
        "\n",
        "    results_path = \"eval_results_finetuned_bert.json\"\n",
        "    preds_path = \"predictions_finetuned_bert.jsonl\"\n",
        "\n",
        "    with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    with open(preds_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, pred in enumerate(preds):\n",
        "            # Build full reference recipe format\n",
        "            full_reference = (\n",
        "                f\"Certainly! Here's a delicious recipe for:\\n\"\n",
        "                f\"[ {val_data[i].get('title', 'Recipe')} ]\\n\\n\"\n",
        "                f\"[ INGREDIENTS ]\\n{val_data[i].get('ingredients', '')}\\n\\n\"\n",
        "                f\"[ DIRECTIONS ]\\n{val_data[i].get('directions', '')}\"\n",
        "            )\n",
        "\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"title\": val_data[i].get(\"title\", \"\"),\n",
        "                    \"NER\": val_data[i].get(\"NER\", \"\"),\n",
        "                    \"ingredients\": val_data[i].get(\"ingredients\", \"\"),\n",
        "                    \"directions\": val_data[i].get(\"directions\", \"\"),\n",
        "                    \"reference_full\": full_reference,\n",
        "                    \"prediction\": pred,\n",
        "                    \"bert_precision\": bert_scores[\"bert_precision_per_sample\"][i],\n",
        "                    \"bert_recall\": bert_scores[\"bert_recall_per_sample\"][i],\n",
        "                    \"bert_f1\": bert_scores[\"bert_f1_per_sample\"][i],\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    print(f\"\\n[INFO] Saved results to: {results_path}\")\n",
        "    print(f\"[INFO] Saved predictions to: {preds_path}\")\n",
        "\n",
        "    return bert_scores, preds\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Main\n",
        "# ---------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"FINE-TUNED MODEL BERT SCORE EVALUATION\")\n",
        "    print(f\"   Fine-tuned Model: {FINETUNED_MODEL_ID}\")\n",
        "    print(f\"   Base Model: {BASE_MODEL}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    bert_scores, predictions = evaluate_finetuned_bert()\n",
        "    print(\"\\n[INFO] Evaluation complete.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINE-TUNED MODEL BERT SCORE RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"  BERT Precision: {bert_scores['bert_precision']:.4f}\")\n",
        "    print(f\"  BERT Recall:    {bert_scores['bert_recall']:.4f}\")\n",
        "    print(f\"  BERT F1:        {bert_scores['bert_f1']:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\n[INFO] Example prediction:\\n\")\n",
        "    print(predictions[0])\n",
        "\n",
        "    print(\"\\n[INFO] Full BERT scores dict:\")\n",
        "    print({k: v for k, v in bert_scores.items() if not k.endswith('_per_sample')})\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
