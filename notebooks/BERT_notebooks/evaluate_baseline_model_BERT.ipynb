{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -q torch tqdm datasets peft transformers bert-score\n",
        "! pip install -U bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from bert_score import score as bert_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASETS_DIR = \"./datasets\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration registry\n",
        "MODEL_CONFIGS = {\n",
        "    \"llama\": {\n",
        "        \"path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "        \"supports_system\": True,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"mistral\": {\n",
        "        \"path\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "        \"supports_system\": False,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"gemma\": {\n",
        "        \"path\": \"ggoogle/gemma-2-2b-it\",\n",
        "        \"supports_system\": False,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"qwen\": {\n",
        "        \"path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        \"supports_system\": True,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"olmo\": {\n",
        "        \"path\": \"allenai/OLMoE-1B-7B-0924-Instruct\",\n",
        "        \"supports_system\": False,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "def get_model_config_from_path(model_path: str):\n",
        "    \"\"\"\n",
        "    Extract model configuration from full model path.\n",
        "\n",
        "    Args:\n",
        "        model_path: Full model path (e.g., \"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing model configuration\n",
        "    \"\"\"\n",
        "    model_path_lower = model_path.lower()\n",
        "\n",
        "    if \"llama\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"llama\"].copy()\n",
        "    elif \"mistral\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"mistral\"].copy()\n",
        "    elif \"gemma\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"gemma\"].copy()\n",
        "    elif \"qwen\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"qwen\"].copy()\n",
        "    elif \"olmo\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"olmo\"].copy()\n",
        "    else:\n",
        "        # Default to Llama format\n",
        "        print(f\"[WARNING] Unknown model path: {model_path}. Using Llama format as default.\")\n",
        "        return MODEL_CONFIGS[\"llama\"].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_subset(dataset, n_samples, seed=42):\n",
        "    \"\"\"\n",
        "    Select a subset of the dataset.\n",
        "    If n_samples is \"all\" or None, return the entire dataset.\n",
        "    Otherwise, sample n_samples examples.\n",
        "    \"\"\"\n",
        "    if n_samples == \"all\" or n_samples is None:\n",
        "        return dataset\n",
        "\n",
        "    if n_samples > len(dataset):\n",
        "        print(f\"[WARNING] Requested {n_samples} samples but only {len(dataset)} available. Using all samples.\")\n",
        "        return dataset\n",
        "\n",
        "    return dataset.shuffle(seed=seed).select(range(n_samples))\n",
        "\n",
        "\n",
        "def load_and_prepare_dataset(cfg):\n",
        "    \"\"\"\n",
        "    Load dataset splits according to configuration.\n",
        "    Ensures the FULL dataset is cached, and subsets are selected per run.\n",
        "    \"\"\"\n",
        "    # Extract dataset configuration\n",
        "    if \"dataset\" in cfg:\n",
        "        cfg_dataset = cfg[\"dataset\"]\n",
        "        dataset_name = cfg_dataset[\"name\"]\n",
        "        splits_cfg = cfg_dataset.get(\"splits\", {})\n",
        "        n_train = splits_cfg.get(\"train\", \"all\")\n",
        "        n_val = splits_cfg.get(\"validation\", \"all\")\n",
        "        n_test = splits_cfg.get(\"test\", \"all\")\n",
        "        seed = cfg_dataset.get(\"seed\", 42)\n",
        "    elif \"datasets\" in cfg and isinstance(cfg[\"datasets\"], list):\n",
        "        cfg_dataset = cfg[\"datasets\"][0]\n",
        "        dataset_name = cfg_dataset[\"path\"]\n",
        "        n_train = cfg.get(\"train_samples\", \"all\")\n",
        "        n_val = cfg.get(\"val_samples\", \"all\")\n",
        "        n_test = cfg.get(\"test_samples\", \"all\")\n",
        "        seed = cfg.get(\"seed\", 42)\n",
        "    else:\n",
        "        raise KeyError(\"Dataset configuration not found. Expected 'dataset' or 'datasets' key.\")\n",
        "\n",
        "    # Load or download full dataset\n",
        "    os.makedirs(DATASETS_DIR, exist_ok=True)\n",
        "    local_path = os.path.join(DATASETS_DIR, dataset_name.replace(\"/\", \"_\"))\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"[INFO] Loading dataset from local cache: {local_path}\")\n",
        "        dataset = load_from_disk(local_path)\n",
        "    else:\n",
        "        print(f\"[INFO] Downloading dataset from Hugging Face: {dataset_name}\")\n",
        "        dataset = load_dataset(dataset_name)\n",
        "        dataset.save_to_disk(local_path)\n",
        "        print(f\"[INFO] Full dataset saved locally to: {local_path}\")\n",
        "\n",
        "    # Filter invalid samples (required for recipe datasets)\n",
        "    def is_valid(sample):\n",
        "        \"\"\"Check if sample has all required fields.\"\"\"\n",
        "        return (\n",
        "            sample.get('title') is not None and str(sample.get('title', '')).strip() and\n",
        "            sample.get('ingredients') is not None and str(sample.get('ingredients', '')).strip() and\n",
        "            sample.get('directions') is not None and str(sample.get('directions', '')).strip() and\n",
        "            sample.get('prompt') is not None and str(sample.get('prompt', '')).strip() and\n",
        "            '[INST]' in str(sample.get('prompt', '')) and '[/INST]' in str(sample.get('prompt', ''))\n",
        "        )\n",
        "\n",
        "    print(\"\\n[INFO] Filtering Invalid Samples:\")\n",
        "    for split_name in dataset.keys():\n",
        "        original_size = len(dataset[split_name])\n",
        "        dataset[split_name] = dataset[split_name].filter(is_valid)\n",
        "        new_size = len(dataset[split_name])\n",
        "        removed = original_size - new_size\n",
        "        print(f\"  {split_name}: kept {new_size:,} / {original_size:,} (removed {removed:,})\")\n",
        "\n",
        "    # Create validation split from training data (if it doesn't exist)\n",
        "    if \"validation\" not in dataset and \"val\" not in dataset:\n",
        "        val_size = cfg_dataset.get(\"val_size\", 0.05)\n",
        "        print(f\"\\n[INFO] Creating Validation Split ({val_size*100:.1f}% of train)\")\n",
        "        train_val_split = dataset['train'].train_test_split(\n",
        "            test_size=val_size,\n",
        "            seed=seed\n",
        "        )\n",
        "        dataset['train'] = train_val_split['train']\n",
        "        dataset['validation'] = train_val_split['test']\n",
        "        print(f\"[INFO] Created validation split: {len(dataset['validation']):,} samples\")\n",
        "\n",
        "    # Handle variations in split keys and select subsets dynamically\n",
        "    val_key = \"validation\" if \"validation\" in dataset else \"val\"\n",
        "\n",
        "    train = select_subset(dataset[\"train\"], n_train, seed=seed)\n",
        "    val = select_subset(dataset[val_key], n_val, seed=seed)\n",
        "    test = select_subset(dataset[\"test\"], n_test, seed=seed)\n",
        "\n",
        "    print(f\"\\n[INFO] Loaded {len(train)} train / {len(val)} val / {len(test)} test samples (from full cache).\")\n",
        "    return train, val, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_model_and_tokenizer(cfg, use_4bit: bool = None, use_lora: bool = None):\n",
        "    \"\"\"\n",
        "    Load model, tokenizer, and apply quantization + LoRA config if specified.\n",
        "    \"\"\"\n",
        "    model_name = cfg[\"base_model\"]\n",
        "    print(f\"\\nLoading model: {model_name}\")\n",
        "\n",
        "    # Tokenizer setup\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    # Determine quantization + LoRA usage\n",
        "    load_in_4bit = use_4bit if use_4bit is not None else cfg.get(\"load_in_4bit\", False)\n",
        "    apply_lora = use_lora if use_lora is not None else (\"lora_r\" in cfg)\n",
        "\n",
        "    # Quantization setup (optional)\n",
        "    quant_cfg = None\n",
        "    if load_in_4bit:\n",
        "        print(\"[INFO] Enabling 4-bit quantization...\")\n",
        "        quant_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=cfg.get(\"bnb_4bit_quant_type\", \"nf4\"),\n",
        "            bnb_4bit_use_double_quant=cfg.get(\"bnb_4bit_use_double_quant\", True),\n",
        "            bnb_4bit_compute_dtype=getattr(\n",
        "                torch, cfg.get(\"bnb_4bit_compute_dtype\", \"bfloat16\")\n",
        "            ),\n",
        "        )\n",
        "    else:\n",
        "        print(\"[INFO] Loading model in full precision (no quantization).\")\n",
        "\n",
        "    # Model loading\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quant_cfg,\n",
        "        device_map=\"auto\",\n",
        "        dtype=(\n",
        "            torch.bfloat16\n",
        "            if cfg.get(\"bf16\", True) and torch.cuda.is_available()\n",
        "            else torch.float32\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # LoRA setup (optional)\n",
        "    if apply_lora:\n",
        "        print(\"[INFO] Applying LoRA configuration...\")\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "        lora_cfg = LoraConfig(\n",
        "            r=cfg.get(\"lora_r\", 8),\n",
        "            lora_alpha=cfg.get(\"lora_alpha\", 16),\n",
        "            target_modules=cfg.get(\"target_modules\", [\"q_proj\", \"v_proj\"]),\n",
        "            lora_dropout=cfg.get(\"lora_dropout\", 0.05),\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "        model = get_peft_model(model, lora_cfg)\n",
        "        model.print_trainable_parameters()\n",
        "    else:\n",
        "        print(\"[INFO] Skipping LoRA setup - using base model only.\")\n",
        "\n",
        "    return model, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG_FILE_PATH = \"/content/matser_config.yaml\"\n",
        "def load_config(config_path: str = CONFIG_FILE_PATH):\n",
        "    \"\"\"\n",
        "    Load and parse a YAML configuration file.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the config file.\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed configuration dictionary.\n",
        "    \"\"\"\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    return cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = load_config(CONFIG_FILE_PATH)\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_predictions(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset,\n",
        "    task_instruction,\n",
        "    cfg=None,\n",
        "    num_samples=None,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=1000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate model predictions for a dataset (recipe directions).\n",
        "    Uses the same message format as the preprocessing notebook.\n",
        "    \"\"\"\n",
        "    if num_samples is not None and num_samples < len(dataset):\n",
        "        dataset = dataset.select(range(num_samples))\n",
        "\n",
        "    # Get field names from config\n",
        "    if cfg is not None:\n",
        "        field_map = cfg.get(\"dataset\", {}).get(\"field_map\", {})\n",
        "        input_field = field_map.get(\"input\", \"NER\")\n",
        "        base_model = cfg.get(\"base_model\", \"\")\n",
        "    else:\n",
        "        input_field = \"NER\"\n",
        "        base_model = \"\"\n",
        "\n",
        "    # Get model config to determine message format\n",
        "    model_config = get_model_config_from_path(base_model)\n",
        "\n",
        "    # Prepare prompts using the same format as preprocessing\n",
        "    prompts = []\n",
        "    for sample in dataset:\n",
        "        messages = []\n",
        "\n",
        "        if model_config['supports_system']:\n",
        "            system_msg = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": model_config['system_message']\n",
        "            }\n",
        "            messages.append(system_msg)\n",
        "\n",
        "            user_content = model_config['user_message_template'].format(ner=sample.get(input_field, ''))\n",
        "            user_msg = {\"role\": \"user\", \"content\": user_content}\n",
        "            messages.append(user_msg)\n",
        "        else:\n",
        "            user_lines = []\n",
        "            user_lines.append(model_config['system_message'])\n",
        "            user_lines.append(\"\")\n",
        "\n",
        "            ner = sample.get(input_field, '')\n",
        "            user_content = model_config['user_message_template'].format(ner=ner)\n",
        "            user_lines.append(user_content)\n",
        "\n",
        "            user_msg = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\\n\\n\".join(user_lines)\n",
        "            }\n",
        "            messages.append(user_msg)\n",
        "\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dtype=\"auto\",\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    # Generate predictions\n",
        "    preds = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating recipes\"):\n",
        "        batch = prompts[i : i + batch_size]\n",
        "        outputs = pipe(batch, max_new_tokens=max_new_tokens, return_full_text=False)\n",
        "        preds.extend([o[0][\"generated_text\"].strip() for o in outputs])\n",
        "\n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_bert_score(predictions, samples, cfg=None, lang=\"en\", model_type=None, batch_size=32):\n",
        "    \"\"\"\n",
        "    Compute BERTScore between predictions and reference full recipe format.\n",
        "    Builds full recipe format from dataset fields (matching preprocessing format).\n",
        "\n",
        "    Args:\n",
        "        predictions (list[str]): Model-generated outputs (full recipe format).\n",
        "        samples (datasets.Dataset): Dataset containing recipe fields (title, ingredients, directions).\n",
        "        cfg (dict, optional): Configuration dictionary (for compatibility).\n",
        "        lang (str): Language for BERTScore (default: \"en\").\n",
        "        model_type (str, optional): Specific BERT model to use (e.g., \"microsoft/deberta-xlarge-mnli\").\n",
        "                                    If None, uses default model for the language (roberta-large).\n",
        "        batch_size (int): Batch size for BERTScore computation (default: 32).\n",
        "\n",
        "    Returns:\n",
        "        dict: BERTScore precision, recall, and F1 scores (averaged and per-sample).\n",
        "    \"\"\"\n",
        "    # Build full recipe format for references (same format as preprocessing)\n",
        "    references = []\n",
        "    for sample in samples:\n",
        "        full_recipe = (\n",
        "            f\"Certainly! Here's a delicious recipe for:\\n\"\n",
        "            f\"[ {sample.get('title', 'Recipe')} ]\\n\\n\"\n",
        "            f\"[ INGREDIENTS ]\\n{sample.get('ingredients', '')}\\n\\n\"\n",
        "            f\"[ DIRECTIONS ]\\n{sample.get('directions', '')}\"\n",
        "        )\n",
        "        references.append(full_recipe)\n",
        "\n",
        "    # Compute BERTScore\n",
        "    print(f\"\\n[INFO] Computing BERTScore with lang='{lang}', model_type='{model_type or 'default (roberta-large)'}', batch_size={batch_size}\")\n",
        "    P, R, F1 = bert_score(\n",
        "        cands=predictions,\n",
        "        refs=references,\n",
        "        lang=lang,\n",
        "        model_type=model_type,\n",
        "        verbose=True,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"bert_precision\": P.mean().item(),\n",
        "        \"bert_recall\": R.mean().item(),\n",
        "        \"bert_f1\": F1.mean().item(),\n",
        "        # Also return per-sample scores for analysis\n",
        "        \"bert_precision_per_sample\": P.tolist(),\n",
        "        \"bert_recall_per_sample\": R.tolist(),\n",
        "        \"bert_f1_per_sample\": F1.tolist(),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "evaluate_baseline_bert.py\n",
        "Evaluate the base (unfine-tuned) model on the recipe generation dataset using BERT Score.\n",
        "\"\"\"\n",
        "\n",
        "cfg = load_config()\n",
        "\n",
        "def evaluate_baseline_bert():\n",
        "    \"\"\"Run baseline evaluation on the recipe generation dataset using BERT Score.\"\"\"\n",
        "\n",
        "    # Load validation data\n",
        "    _, val_data, _ = load_and_prepare_dataset(cfg)\n",
        "    print(f\"[INFO] Loaded {len(val_data)} validation samples.\")\n",
        "\n",
        "    # Load model + tokenizer (no quantization or LoRA)\n",
        "    model, tokenizer = setup_model_and_tokenizer(\n",
        "        cfg=cfg,\n",
        "        use_4bit=False,\n",
        "        use_lora=False,\n",
        "    )\n",
        "\n",
        "    # Generate predictions\n",
        "    print(\"\\n[INFO] Generating predictions...\")\n",
        "    preds = generate_predictions(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=val_data,\n",
        "        task_instruction=cfg[\"task_instruction\"],\n",
        "        cfg=cfg,\n",
        "        batch_size=4,\n",
        "    )\n",
        "\n",
        "    # Compute BERT scores\n",
        "    print(\"\\n[INFO] Computing BERT scores...\")\n",
        "    bert_scores = compute_bert_score(preds, val_data, cfg=cfg)\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Save outputs\n",
        "    # -----------------------------------------------------------------------\n",
        "    results = {\n",
        "        \"model_name\": cfg[\"base_model\"],\n",
        "        \"num_samples\": len(val_data),\n",
        "        \"bert_precision\": bert_scores[\"bert_precision\"],\n",
        "        \"bert_recall\": bert_scores[\"bert_recall\"],\n",
        "        \"bert_f1\": bert_scores[\"bert_f1\"],\n",
        "    }\n",
        "\n",
        "    results_path = \"eval_results_bert.json\"\n",
        "    preds_path = \"predictions_bert.jsonl\"\n",
        "\n",
        "    with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    with open(preds_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, pred in enumerate(preds):\n",
        "            # Build full reference recipe format\n",
        "            full_reference = (\n",
        "                f\"Certainly! Here's a delicious recipe for:\\n\"\n",
        "                f\"[ {val_data[i].get('title', 'Recipe')} ]\\n\\n\"\n",
        "                f\"[ INGREDIENTS ]\\n{val_data[i].get('ingredients', '')}\\n\\n\"\n",
        "                f\"[ DIRECTIONS ]\\n{val_data[i].get('directions', '')}\"\n",
        "            )\n",
        "\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"title\": val_data[i].get(\"title\", \"\"),\n",
        "                    \"NER\": val_data[i].get(\"NER\", \"\"),\n",
        "                    \"ingredients\": val_data[i].get(\"ingredients\", \"\"),\n",
        "                    \"directions\": val_data[i].get(\"directions\", \"\"),\n",
        "                    \"reference_full\": full_reference,\n",
        "                    \"prediction\": pred,\n",
        "                    \"bert_precision\": bert_scores[\"bert_precision_per_sample\"][i],\n",
        "                    \"bert_recall\": bert_scores[\"bert_recall_per_sample\"][i],\n",
        "                    \"bert_f1\": bert_scores[\"bert_f1_per_sample\"][i],\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "            f.write(\"\\n\")\n",
        "    print(f\"\\n[INFO] Saved results to: {results_path}\")\n",
        "    print(f\"[INFO] Saved predictions to: {preds_path}\")\n",
        "\n",
        "    return bert_scores, preds\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Main\n",
        "# ---------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"[INFO] Starting baseline BERT Score evaluation...\")\n",
        "    bert_scores, predictions = evaluate_baseline_bert()\n",
        "    print(\"\\n[INFO] Evaluation complete.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BASELINE BERT SCORE RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"  BERT Precision: {bert_scores['bert_precision']:.4f}\")\n",
        "    print(f\"  BERT Recall:    {bert_scores['bert_recall']:.4f}\")\n",
        "    print(f\"  BERT F1:        {bert_scores['bert_f1']:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\n[INFO] Example prediction:\\n\")\n",
        "    print(predictions[0])\n",
        "\n",
        "    print(\"\\n[INFO] Full BERT scores dict:\")\n",
        "    print({k: v for k, v in bert_scores.items() if not k.endswith('_per_sample')})\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
