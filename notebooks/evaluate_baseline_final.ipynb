{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c-HKFuB4zKpk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-HKFuB4zKpk",
        "outputId": "57f47007-e774-48fb-bb43-b55898b1ae71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q evaluate torch tqdm datasets peft transformers rouge_score\n",
        "! pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73828e7d",
      "metadata": {
        "id": "73828e7d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import json\n",
        "import torch\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1480d0",
      "metadata": {
        "id": "1e1480d0"
      },
      "outputs": [],
      "source": [
        "DATASETS_DIR = \"./datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5222f31d",
      "metadata": {
        "id": "5222f31d"
      },
      "outputs": [],
      "source": [
        "# Model configuration registry\n",
        "MODEL_CONFIGS = {\n",
        "    \"llama\": {\n",
        "        \"path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "        \"supports_system\": True,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"mistral\": {\n",
        "        \"path\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "        \"supports_system\": False,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "\n",
        "    },\n",
        "    \"gemma\": {\n",
        "        \"path\": \"google/gemma-2-9b-it\",\n",
        "        \"supports_system\": False,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"qwen\": {\n",
        "        \"path\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        \"supports_system\": True,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "    \"olmo\": {\n",
        "        \"path\": \"allenai/OLMoE-1B-7B-0924-Instruct\",\n",
        "        \"supports_system\": False,\n",
        "        \"system_message\": \"You will generate one cooking recipe. List all necessary ingredients and give detailed steps.\",\n",
        "        \"user_message_template\": \"Include ingredients: {ner}\",\n",
        "        \"include_title_in_user\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "def get_model_config_from_path(model_path: str):\n",
        "    \"\"\"\n",
        "    Extract model configuration from full model path.\n",
        "\n",
        "    Args:\n",
        "        model_path: Full model path (e.g., \"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing model configuration\n",
        "    \"\"\"\n",
        "    model_path_lower = model_path.lower()\n",
        "\n",
        "    if \"llama\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"llama\"].copy()\n",
        "    elif \"mistral\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"mistral\"].copy()\n",
        "    elif \"gemma\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"gemma\"].copy()\n",
        "    elif \"qwen\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"qwen\"].copy()\n",
        "    elif \"olmo\" in model_path_lower:\n",
        "        return MODEL_CONFIGS[\"olmo\"].copy()\n",
        "    else:\n",
        "        # Default to Llama format\n",
        "        print(f\"[WARNING] Unknown model path: {model_path}. Using Llama format as default.\")\n",
        "        return MODEL_CONFIGS[\"llama\"].copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81981770",
      "metadata": {
        "id": "81981770"
      },
      "outputs": [],
      "source": [
        "def select_subset(dataset, n_samples, seed=42):\n",
        "    \"\"\"\n",
        "    Select a subset of the dataset.\n",
        "    If n_samples is \"all\" or None, return the entire dataset.\n",
        "    Otherwise, sample n_samples examples.\n",
        "    \"\"\"\n",
        "    if n_samples == \"all\" or n_samples is None:\n",
        "        return dataset\n",
        "\n",
        "    if n_samples > len(dataset):\n",
        "        print(f\"[WARNING] Requested {n_samples} samples but only {len(dataset)} available. Using all samples.\")\n",
        "        return dataset\n",
        "\n",
        "    return dataset.shuffle(seed=seed).select(range(n_samples))\n",
        "\n",
        "\n",
        "def load_and_prepare_dataset(cfg):\n",
        "    \"\"\"\n",
        "    Load dataset splits according to configuration.\n",
        "    Ensures the FULL dataset is cached, and subsets are selected per run.\n",
        "    Supports both new-style (\"dataset\": {\"splits\": {...}}) and old-style (top-level keys) configs.\n",
        "    Filters invalid samples and creates validation split if missing (for recipe datasets).\n",
        "    \"\"\"\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Extract dataset configuration\n",
        "    # -----------------------------------------------------------------------\n",
        "    if \"dataset\" in cfg:\n",
        "        cfg_dataset = cfg[\"dataset\"]\n",
        "        dataset_name = cfg_dataset[\"name\"]\n",
        "        splits_cfg = cfg_dataset.get(\"splits\", {})\n",
        "        n_train = splits_cfg.get(\"train\", \"all\")\n",
        "        n_val = splits_cfg.get(\"validation\", \"all\")\n",
        "        n_test = splits_cfg.get(\"test\", \"all\")\n",
        "        seed = cfg_dataset.get(\"seed\", 42)\n",
        "    elif \"datasets\" in cfg and isinstance(cfg[\"datasets\"], list):\n",
        "        cfg_dataset = cfg[\"datasets\"][0]\n",
        "        dataset_name = cfg_dataset[\"path\"]\n",
        "        n_train = cfg.get(\"train_samples\", \"all\")\n",
        "        n_val = cfg.get(\"val_samples\", \"all\")\n",
        "        n_test = cfg.get(\"test_samples\", \"all\")\n",
        "        seed = cfg.get(\"seed\", 42)\n",
        "    else:\n",
        "        raise KeyError(\"Dataset configuration not found. Expected 'dataset' or 'datasets' key.\")\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Load or download full dataset\n",
        "    # -----------------------------------------------------------------------\n",
        "    os.makedirs(DATASETS_DIR, exist_ok=True)\n",
        "    local_path = os.path.join(DATASETS_DIR, dataset_name.replace(\"/\", \"_\"))\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"[INFO] Loading dataset from local cache: {local_path}\")\n",
        "        dataset = load_from_disk(local_path)\n",
        "    else:\n",
        "        print(f\"[INFO] Downloading dataset from Hugging Face: {dataset_name}\")\n",
        "        dataset = load_dataset(dataset_name)\n",
        "        dataset.save_to_disk(local_path)\n",
        "        print(f\"[INFO] Full dataset saved locally to: {local_path}\")\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Filter invalid samples (required for recipe datasets)\n",
        "    # -----------------------------------------------------------------------\n",
        "    def is_valid(sample):\n",
        "        \"\"\"Check if sample has all required fields.\"\"\"\n",
        "        return (\n",
        "            sample.get('title') is not None and str(sample.get('title', '')).strip() and\n",
        "            sample.get('ingredients') is not None and str(sample.get('ingredients', '')).strip() and\n",
        "            sample.get('directions') is not None and str(sample.get('directions', '')).strip() and\n",
        "            sample.get('prompt') is not None and str(sample.get('prompt', '')).strip() and\n",
        "            '[INST]' in str(sample.get('prompt', '')) and '[/INST]' in str(sample.get('prompt', ''))\n",
        "        )\n",
        "\n",
        "    print(\"\\n[INFO] Filtering Invalid Samples:\")\n",
        "    for split_name in dataset.keys():\n",
        "        original_size = len(dataset[split_name])\n",
        "        dataset[split_name] = dataset[split_name].filter(is_valid)\n",
        "        new_size = len(dataset[split_name])\n",
        "        removed = original_size - new_size\n",
        "        print(f\"  {split_name}: kept {new_size:,} / {original_size:,} (removed {removed:,})\")\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Create validation split from training data (if it doesn't exist)\n",
        "    # -----------------------------------------------------------------------\n",
        "    if \"validation\" not in dataset and \"val\" not in dataset:\n",
        "        val_size = cfg_dataset.get(\"val_size\", 0.05)\n",
        "        print(f\"\\n[INFO] Creating Validation Split ({val_size*100:.1f}% of train)\")\n",
        "        train_val_split = dataset['train'].train_test_split(\n",
        "            test_size=val_size,\n",
        "            seed=seed\n",
        "        )\n",
        "        dataset['train'] = train_val_split['train']\n",
        "        dataset['validation'] = train_val_split['test']\n",
        "        print(f\"[INFO] Created validation split: {len(dataset['validation']):,} samples\")\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Handle variations in split keys and select subsets dynamically\n",
        "    # -----------------------------------------------------------------------\n",
        "    val_key = \"validation\" if \"validation\" in dataset else \"val\"\n",
        "\n",
        "    train = select_subset(dataset[\"train\"], n_train, seed=seed)\n",
        "    val = select_subset(dataset[val_key], n_val, seed=seed)\n",
        "    test = select_subset(dataset[\"test\"], n_test, seed=seed)\n",
        "\n",
        "    print(f\"\\n[INFO] Loaded {len(train)} train / {len(val)} val / {len(test)} test samples (from full cache).\")\n",
        "    return train, val, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655fed70",
      "metadata": {
        "id": "655fed70"
      },
      "outputs": [],
      "source": [
        "def setup_model_and_tokenizer(cfg, use_4bit: bool = None, use_lora: bool = None):\n",
        "    \"\"\"\n",
        "    Load model, tokenizer, and apply quantization + LoRA config if specified.\n",
        "\n",
        "    Args:\n",
        "        cfg (dict): Configuration dictionary containing:\n",
        "            - base_model\n",
        "            - quantization parameters\n",
        "            - lora parameters (optional)\n",
        "            - bf16 or fp16 precision\n",
        "        use_4bit (bool, optional): Override whether to load in 4-bit mode.\n",
        "        use_lora (bool, optional): Override whether to apply LoRA adapters.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "    model_name = cfg[\"base_model\"]\n",
        "    print(f\"\\nLoading model: {model_name}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Tokenizer setup\n",
        "    # ------------------------------\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    # Determine quantization + LoRA usage\n",
        "    load_in_4bit = use_4bit if use_4bit is not None else cfg.get(\"load_in_4bit\", False)\n",
        "    apply_lora = use_lora if use_lora is not None else (\"lora_r\" in cfg)\n",
        "\n",
        "    # ------------------------------\n",
        "    # Quantization setup (optional)\n",
        "    # ------------------------------\n",
        "    quant_cfg = None\n",
        "    if load_in_4bit:\n",
        "        print(\"[INFO] Enabling 4-bit quantization...\")\n",
        "        quant_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=cfg.get(\"bnb_4bit_quant_type\", \"nf4\"),\n",
        "            bnb_4bit_use_double_quant=cfg.get(\"bnb_4bit_use_double_quant\", True),\n",
        "            bnb_4bit_compute_dtype=getattr(\n",
        "                torch, cfg.get(\"bnb_4bit_compute_dtype\", \"bfloat16\")\n",
        "            ),\n",
        "        )\n",
        "    else:\n",
        "        print(\"[INFO] Loading model in full precision (no quantization).\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Model loading\n",
        "    # ------------------------------\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quant_cfg,\n",
        "        device_map=\"auto\",\n",
        "        dtype=(\n",
        "            torch.bfloat16\n",
        "            if cfg.get(\"bf16\", True) and torch.cuda.is_available()\n",
        "            else torch.float32\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # ------------------------------\n",
        "    # LoRA setup (optional)\n",
        "    # ------------------------------\n",
        "    if apply_lora:\n",
        "        print(\"[INFO] Applying LoRA configuration...\")\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "        lora_cfg = LoraConfig(\n",
        "            r=cfg.get(\"lora_r\", 8),\n",
        "            lora_alpha=cfg.get(\"lora_alpha\", 16),\n",
        "            target_modules=cfg.get(\"target_modules\", [\"q_proj\", \"v_proj\"]),\n",
        "            lora_dropout=cfg.get(\"lora_dropout\", 0.05),\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "        model = get_peft_model(model, lora_cfg)\n",
        "        model.print_trainable_parameters()\n",
        "    else:\n",
        "        print(\"ðŸ”¹ Skipping LoRA setup â€” using base model only.\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660ef9fe",
      "metadata": {
        "id": "660ef9fe"
      },
      "outputs": [],
      "source": [
        "CONFIG_FILE_PATH = \"/content/config(1).yaml\"\n",
        "def load_config(config_path: str = CONFIG_FILE_PATH):\n",
        "    \"\"\"\n",
        "    Load and parse a YAML configuration file.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the config file.\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed configuration dictionary.\n",
        "    \"\"\"\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    return cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9df4e2",
      "metadata": {
        "id": "7b9df4e2"
      },
      "outputs": [],
      "source": [
        "def generate_predictions(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset,\n",
        "    task_instruction,\n",
        "    cfg=None,\n",
        "    num_samples=None,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=256,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate model predictions for a dataset (recipe directions).\n",
        "    Uses the same message format as the preprocessing notebook.\n",
        "\n",
        "    Args:\n",
        "        model: The loaded model (base or fine-tuned).\n",
        "        tokenizer: Corresponding tokenizer.\n",
        "        dataset: Hugging Face dataset split containing recipe fields (NER, title, ingredients, directions).\n",
        "        task_instruction (str): Instruction prefix (kept for compatibility, not used directly).\n",
        "        cfg (dict, optional): Configuration dictionary to get field_map and base_model.\n",
        "        num_samples (int, optional): Number of samples to evaluate.\n",
        "        batch_size (int): Number of examples per inference batch.\n",
        "        max_new_tokens (int): Max tokens to generate per sample.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: Generated recipe responses (full format).\n",
        "    \"\"\"\n",
        "    if num_samples is not None and num_samples < len(dataset):\n",
        "        dataset = dataset.select(range(num_samples))\n",
        "\n",
        "    # Get field names from config\n",
        "    if cfg is not None:\n",
        "        field_map = cfg.get(\"dataset\", {}).get(\"field_map\", {})\n",
        "        input_field = field_map.get(\"input\", \"NER\")  # Default to NER for recipes\n",
        "        base_model = cfg.get(\"base_model\", \"\")\n",
        "    else:\n",
        "        input_field = \"NER\"  # Default fallback\n",
        "        base_model = \"\"\n",
        "\n",
        "    # Get model config to determine message format (same as preprocessing)\n",
        "    model_config = get_model_config_from_path(base_model)\n",
        "\n",
        "    # Prepare prompts using the same format as preprocessing\n",
        "    prompts = []\n",
        "    for sample in dataset:\n",
        "        messages = []\n",
        "\n",
        "        # Build messages according to model type (same as preprocessing)\n",
        "        if model_config['supports_system']:\n",
        "            # Models with system message support: separate system and user\n",
        "            system_msg = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": model_config['system_message']\n",
        "            }\n",
        "            messages.append(system_msg)\n",
        "\n",
        "            # User message with ingredients\n",
        "            user_content = model_config['user_message_template'].format(ner=sample.get(input_field, ''))\n",
        "            user_msg = {\"role\": \"user\", \"content\": user_content}\n",
        "            messages.append(user_msg)\n",
        "\n",
        "        else:\n",
        "            # Models without system support: merge system into user message\n",
        "            user_lines = []\n",
        "            user_lines.append(model_config['system_message'])\n",
        "            user_lines.append(\"\")\n",
        "\n",
        "            # Build user message with ingredients only (no title)\n",
        "            ner = sample.get(input_field, '')\n",
        "            user_content = model_config['user_message_template'].format(ner=ner)\n",
        "            user_lines.append(user_content)\n",
        "\n",
        "            user_msg = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\\n\\n\".join(user_lines)\n",
        "            }\n",
        "            messages.append(user_msg)\n",
        "\n",
        "        # Apply chat template (same as preprocessing, with generation prompt)\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dtype=\"auto\",\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    # Generate predictions\n",
        "    preds = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating recipes\"):\n",
        "        batch = prompts[i : i + batch_size]\n",
        "        outputs = pipe(batch, max_new_tokens=max_new_tokens, return_full_text=False)\n",
        "        preds.extend([o[0][\"generated_text\"].strip() for o in outputs])\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def compute_rouge(predictions, samples, cfg=None):\n",
        "    \"\"\"\n",
        "    Compute ROUGE scores between predictions and reference full recipe format.\n",
        "    Builds full recipe format from dataset fields (matching preprocessing format).\n",
        "\n",
        "    Args:\n",
        "        predictions (list[str]): Model-generated outputs (full recipe format).\n",
        "        samples (datasets.Dataset): Dataset containing recipe fields (title, ingredients, directions).\n",
        "        cfg (dict, optional): Configuration dictionary (for compatibility).\n",
        "\n",
        "    Returns:\n",
        "        dict: ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
        "    \"\"\"\n",
        "    # Build full recipe format for references (same format as preprocessing)\n",
        "    references = []\n",
        "    for sample in samples:\n",
        "        full_recipe = (\n",
        "            f\"Certainly! Here's a delicious recipe for:\\n\"\n",
        "            f\"[ {sample.get('title', 'Recipe')} ]\\n\\n\"\n",
        "            f\"[ INGREDIENTS ]\\n{sample.get('ingredients', '')}\\n\\n\"\n",
        "            f\"[ DIRECTIONS ]\\n{sample.get('directions', '')}\"\n",
        "        )\n",
        "        references.append(full_recipe)\n",
        "\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    return rouge.compute(predictions=predictions, references=references)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24340d4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746,
          "referenced_widgets": [
            "f95407363995485da2dcdab50fcce8ca",
            "b413844b5f364139b6f4c7d71195b624",
            "f51a148110054fb4ad1473bf0b833dfb",
            "f96590b680a44724a0e6990c349c0524",
            "bce273ff970b4196bdc71450d8a21f0d",
            "30ea829529264a84bb6387827095fec3",
            "0b01bfb3121f453381ed5da769661a81",
            "37c24dcb0be84c958fd43edacf45c09c",
            "926323131e2e4fae8cae92c6ac14afe8",
            "5f32dc759e5c476387b3464a67a29270",
            "a8e62a47e33647d3b449a2ef7b76e226",
            "d66ebb253e0741e28c3b2c1b6509f8c5",
            "aa67e1f84a6f4219b6b0aa03d369b860",
            "c7a3a2c20d4d45458604d24b31c5ed8c",
            "6b7a367a70a1445cabc53829f58bcc5c",
            "36529ac8d5074aea9351c2f6e991a348",
            "cb06e7b35eaf47f08a00fb64022c8950",
            "ea3cc14329c64ae89959e8d136add078",
            "e5fade6cc8a242878717c63844928b2e",
            "d90574fa052d428c9929bd781034959c",
            "d46a6e4b78f34bc59110a125675caf86"
          ]
        },
        "id": "24340d4c",
        "outputId": "fc51ec9b-a0f9-44c8-804b-c289f588e1f1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Starting baseline evaluation...\n",
            "[INFO] Downloading dataset from Hugging Face: skadewdl3/recipe-nlg-llama2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f95407363995485da2dcdab50fcce8ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/822 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b413844b5f364139b6f4c7d71195b624",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00007-73f49c89fca0ad(â€¦):   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f51a148110054fb4ad1473bf0b833dfb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00001-of-00007-1d10e9ac88b427(â€¦):   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f96590b680a44724a0e6990c349c0524",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00002-of-00007-959831b6c5f145(â€¦):   0%|          | 0.00/218M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bce273ff970b4196bdc71450d8a21f0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00003-of-00007-d6f01c4aafdff2(â€¦):   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30ea829529264a84bb6387827095fec3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00004-of-00007-7f66de25de3e0e(â€¦):   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b01bfb3121f453381ed5da769661a81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00005-of-00007-2725711559578e(â€¦):   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37c24dcb0be84c958fd43edacf45c09c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00006-of-00007-27897e3bd5e7ae(â€¦):   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "926323131e2e4fae8cae92c6ac14afe8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/test-00000-of-00001-f0fb9d9a114d945(â€¦):   0%|          | 0.00/169M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f32dc759e5c476387b3464a67a29270",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/2008027 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8e62a47e33647d3b449a2ef7b76e226",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/223115 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d66ebb253e0741e28c3b2c1b6509f8c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/7 shards):   0%|          | 0/2008027 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa67e1f84a6f4219b6b0aa03d369b860",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/223115 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Full dataset saved locally to: ./datasets/skadewdl3_recipe-nlg-llama2\n",
            "\n",
            "[INFO] Filtering Invalid Samples:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7a3a2c20d4d45458604d24b31c5ed8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/2008027 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  train: kept 2,008,026 / 2,008,027 (removed 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b7a367a70a1445cabc53829f58bcc5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/223115 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  test: kept 223,115 / 223,115 (removed 0)\n",
            "\n",
            "[INFO] Creating Validation Split (5.0% of train)\n",
            "[INFO] Created validation split: 100,402 samples\n",
            "\n",
            "[INFO] Loaded 1907624 train / 200 val / 200 test samples (from full cache).\n",
            "[INFO] Loaded 200 validation samples.\n",
            "\n",
            "Loading model: meta-llama/Llama-3.2-1B-Instruct\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36529ac8d5074aea9351c2f6e991a348",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb06e7b35eaf47f08a00fb64022c8950",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea3cc14329c64ae89959e8d136add078",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Loading model in full precision (no quantization).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5fade6cc8a242878717c63844928b2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d90574fa052d428c9929bd781034959c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d46a6e4b78f34bc59110a125675caf86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”¹ Skipping LoRA setup â€” using base model only.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating recipes:  20%|â–ˆâ–ˆ        | 10/50 [09:37<37:59, 57.00s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Generating recipes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [42:34<02:39, 53.24s/it]"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "evaluate_baseline.py\n",
        "Evaluate the base (unfine-tuned) model on the recipe generation dataset to establish baseline ROUGE scores.\n",
        "\"\"\"\n",
        "\n",
        "cfg = load_config()\n",
        "\n",
        "def evaluate_baseline():\n",
        "    \"\"\"Run baseline evaluation on the recipe generation dataset using the base model.\"\"\"\n",
        "\n",
        "    # Load validation data\n",
        "    _, val_data, _ = load_and_prepare_dataset(cfg)\n",
        "    print(f\"[INFO] Loaded {len(val_data)} validation samples.\")\n",
        "\n",
        "    # Load model + tokenizer (no quantization or LoRA)\n",
        "    model, tokenizer = setup_model_and_tokenizer(\n",
        "        cfg=cfg,\n",
        "        use_4bit=False,\n",
        "        use_lora=False,\n",
        "    )\n",
        "\n",
        "    # Generate predictions\n",
        "    preds = generate_predictions(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=val_data,\n",
        "        task_instruction=cfg[\"task_instruction\"],\n",
        "        cfg=cfg,\n",
        "        batch_size=4,\n",
        "    )\n",
        "\n",
        "    # Compute ROUGE metrics\n",
        "    scores = compute_rouge(preds, val_data, cfg=cfg)\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Save outputs\n",
        "    # -----------------------------------------------------------------------\n",
        "    results = {\n",
        "        \"model_name\": cfg[\"base_model\"],\n",
        "        \"num_samples\": len(val_data),\n",
        "        \"rouge1\": scores[\"rouge1\"],\n",
        "        \"rouge2\": scores[\"rouge2\"],\n",
        "        \"rougeL\": scores[\"rougeL\"],\n",
        "    }\n",
        "\n",
        "    results_path = \"eval_results.json\"\n",
        "    preds_path = \"predictions.jsonl\"\n",
        "\n",
        "    with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    with open(preds_path, \"w\", encoding=\"utf-8\") as f:  # Fixed: removed 4 spaces\n",
        "        for i, pred in enumerate(preds):\n",
        "            # Build full reference recipe format\n",
        "            full_reference = (\n",
        "                f\"Certainly! Here's a delicious recipe for:\\n\"\n",
        "                f\"[ {val_data[i].get('title', 'Recipe')} ]\\n\\n\"\n",
        "                f\"[ INGREDIENTS ]\\n{val_data[i].get('ingredients', '')}\\n\\n\"\n",
        "                f\"[ DIRECTIONS ]\\n{val_data[i].get('directions', '')}\"\n",
        "            )\n",
        "\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"title\": val_data[i].get(\"title\", \"\"),\n",
        "                    \"NER\": val_data[i].get(\"NER\", \"\"),\n",
        "                    \"ingredients\": val_data[i].get(\"ingredients\", \"\"),\n",
        "                    \"directions\": val_data[i].get(\"directions\", \"\"),\n",
        "                    \"reference_full\": full_reference,\n",
        "                    \"prediction\": pred,\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "            f.write(\"\\n\")\n",
        "    print(f\"\\n[INFO] Saved results to: {results_path}\")\n",
        "    print(f\"[INFO] Saved predictions to: {preds_path}\")\n",
        "\n",
        "    return scores, preds\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Main\n",
        "# ---------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"[INFO] Starting baseline evaluation...\")\n",
        "    rouge_scores, predictions = evaluate_baseline()\n",
        "    print(\"\\n[INFO] Evaluation complete.\")\n",
        "\n",
        "\n",
        "    print(\"\\n[INFO] Baseline ROUGE Results:\")\n",
        "    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.2%}\")\n",
        "    print(f\"  ROUGE-2: {rouge_scores['rouge2']:.2%}\")\n",
        "    print(f\"  ROUGE-L: {rouge_scores['rougeL']:.2%}\")\n",
        "\n",
        "    print(\"\\nExample prediction:\\n\")\n",
        "    print(predictions[0])\n",
        "    print(\"\\nRouge scores:\\n\")\n",
        "    print(rouge_scores)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ja70tdWgXeM5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja70tdWgXeM5",
        "outputId": "56de3c34-dc7c-4d99-c7cc-3d3cea877d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfede576",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfede576",
        "outputId": "1c874870-8cd6-42f6-8968-b11e73d0509a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N1oznuuvYUU4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "N1oznuuvYUU4",
        "outputId": "e3d7f542-21da-41b6-a797-f45cb2987701"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ab0f1285-8139-4a6e-9174-56be83dfc8da\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ab0f1285-8139-4a6e-9174-56be83dfc8da\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving config.yaml to config.yaml\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccQpY8niYfKb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccQpY8niYfKb",
        "outputId": "890cf94e-5d1f-424e-ff98-57a6372dbc69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.yaml  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}